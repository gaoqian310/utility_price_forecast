# -*- coding: utf-8 -*-
"""AESO API.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wGFmpsaC32k8QQVgL7_0ghg5yWNZfnR3
"""

import pandas as pd
import numpy as np
import requests
import json
from datetime import datetime

### Setup parameters to pass into requests.get() function. Parameters include (URL), (params), (headers), (auth), etc... ###

# URL Parameter
api_url = 'https://api.aeso.ca/report/v1.1/price/poolPrice?'
print(api_url)

# Headers parameter
headers = {'accept': 'application/json',
           'X-API-Key':'eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJwZ3Nqa2UiLCJpYXQiOjE3MDc1MDYyNzl9._mC8xDXPUW9LI4OkMlYh7NOs0SjnZnu30seW4pcZFWY'}
           # Why is the API key in the header? And not in auth?

# Params parameter - Note date range is limited to 1 year per query
params = {
    'startDate':'2023-12-01',
    'endDate': '2024-02-01'
    }

# Get data. Output = requests.Response object (has its own parts)
response_API = requests.get(api_url, params=params, headers=headers)
print(f"Response status code: {response_API.status_code}")



### Convert json to dataframe ###

json_data = response_API.json()

# The data, 'Pool Price Report', is nested within 'return'
df = pd.json_normalize(json_data['return']['Pool Price Report'])


### AGGREGATE FROM HOURLY TO DAILY ###

# Convert dates to datetime format
df['begin_datetime_mpt'] = pd.to_datetime(df['begin_datetime_mpt'])

# Convert pool_price to float
df['pool_price'] = pd.to_numeric(df['pool_price'], errors='coerce')

# Set 'begin_datetime_mpt' as the index
df.set_index('begin_datetime_mpt', inplace=True)


# Group by daily and aggregate using mean (you can use 'sum' instead of 'mean' if needed)
# daily_aggregated_df = df.resample('D').mean()
daily_aggregated_df = df.resample('D').agg({
    'pool_price': ['min', 'max', 'mean']
})

# Reset index to make 'begin_datetime_mpt' a column again
daily_aggregated_df.reset_index(inplace=True)

print(daily_aggregated_df)

# Convert to CSV
daily_aggregated_df.to_csv('AESO Pool Price - Aggregated Daily.csv', index=False)

### BELOW IS TO RUN THE ABOVE CODE OVER A RANGE OF YEARS ###

# Params parameter - Note date range is limited to 1 year per query
start_year = 2000 # CHANGE THIS
end_year = 2024 # AND THIS
year_list = range(start_year, end_year + 1)

# Create datetime objects for the start and end of each year
date_ranges = []

for year in year_list:
    start_date = datetime(year, 1, 1)
    end_date = datetime(year, 12, 31)
    date_ranges.append((start_date, end_date))


# Retrieve data for all years in date_ranges
data_frames = [] # Create a list

for start_date, end_date in date_ranges:
    params = {
    'startDate':{start_date.date()},
    'endDate': {end_date.date()}
    }

    print(f"Start Date: {start_date.date()}, End Date: {end_date.date()}")
    response_API = requests.get(api_url, params=params, headers=headers)
    print(f"Response status code: {response_API.status_code}")

    json_data = response_API.json() # Get data
    df1year = pd.json_normalize(json_data['return']['Pool Price Report']) # Convert to df
    data_frames.append(df1year) # Append years to list

df = pd.concat(data_frames, ignore_index=True) # Convert list to dataframe?

# Convert dates to datetime format
df['begin_datetime_mpt'] = pd.to_datetime(df['begin_datetime_mpt'])

# Convert pool_price to float
df['pool_price'] = pd.to_numeric(df['pool_price'], errors='coerce')

# Set 'begin_datetime_mpt' as the index
df.set_index('begin_datetime_mpt', inplace=True)


# Group by daily and aggregate using mean (you can use 'sum' instead of 'mean' if needed)
# daily_aggregated_df = df.resample('D').mean()
daily_aggregated_df = df.resample('D').agg({
    'pool_price': ['min', 'max', 'mean']
})

# Reset index to make 'begin_datetime_mpt' a column again
daily_aggregated_df.reset_index(inplace=True)

print(daily_aggregated_df)

# Convert to CSV
daily_aggregated_df.to_csv('AESO Pool Price - Date Range, Aggregated Daily.csv', index=False)